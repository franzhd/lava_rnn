{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import typing as ty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lava.magma.core.process.ports.ports.reduce_ops'; 'lava.magma.core.process.ports.ports' is not a package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmagma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvariable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Var\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmagma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mports\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InPort, OutPort\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmagma\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mports\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreduce_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReduceS\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdense\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlava\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mproc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlif\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningLIF\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lava.magma.core.process.ports.ports.reduce_ops'; 'lava.magma.core.process.ports.ports' is not a package"
     ]
    }
   ],
   "source": [
    "# Import Process level primitives\n",
    "from lava.magma.core.process.process import AbstractProcess\n",
    "from lava.magma.core.process.variable import Var\n",
    "from lava.magma.core.process.ports.ports import InPort, OutPort\n",
    "from lava.magma.core.process.ports.reduce_ops.reduce_ops import ReduceSum\n",
    "from lava.proc.dense.process import Dense\n",
    "from lava.proc.lif.process import LearningLIF\n",
    "from lava.proc.learning_rules.stdp_learning_rule import STDPLoihi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mLIFRefractory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Leaky-Integrate-and-Fire (LIF) process with refractory period.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "shape : tuple(int)\n",
      "    Number and topology of LIF neurons.\n",
      "u : float, list, numpy.ndarray, optional\n",
      "    Initial value of the neurons' current.\n",
      "v : float, list, numpy.ndarray, optional\n",
      "    Initial value of the neurons' voltage (membrane potential).\n",
      "du : float, optional\n",
      "    Inverse of decay time-constant for current decay. Currently, only a\n",
      "    single decay can be set for the entire population of neurons.\n",
      "dv : float, optional\n",
      "    Inverse of decay time-constant for voltage decay. Currently, only a\n",
      "    single decay can be set for the entire population of neurons.\n",
      "bias_mant : float, list, numpy.ndarray, optional\n",
      "    Mantissa part of neuron bias.\n",
      "bias_exp : float, list, numpy.ndarray, optional\n",
      "    Exponent part of neuron bias, if needed. Mostly for fixed point\n",
      "    implementations. Ignored for floating point implementations.\n",
      "vth : float, optional\n",
      "    Neuron threshold voltage, exceeding which, the neuron will spike.\n",
      "    Currently, only a single threshold can be set for the entire\n",
      "    population of neurons.\n",
      "refractory_period : int, optional\n",
      "    The interval of the refractory period. 1 timestep by default.\n",
      "\n",
      "\n",
      "See Also\n",
      "--------\n",
      "lava.proc.lif.process.LIF: 'Regular' leaky-integrate-and-fire neuron for\n",
      "documentation on rest of the behavior.\n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes a new Process.\n",
      "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/lava/proc/lif/process.py\n",
      "\u001b[0;31mType:\u001b[0m           ProcessPostInitCaller\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "LIFRefractory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Dense connections between neurons. Realizes the following abstract\n",
      "behavior: a_out = weights * s_in\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "weights : numpy.ndarray\n",
      "    2D connection weight matrix of form (num_flat_output_neurons,\n",
      "    num_flat_input_neurons) in C-order (row major).\n",
      "\n",
      "weight_exp : int, optional\n",
      "    Shared weight exponent of base 2 used to scale magnitude of\n",
      "    weights, if needed. Mostly for fixed point implementations.\n",
      "    Unnecessary for floating point implementations.\n",
      "    Default value is 0.\n",
      "\n",
      "num_weight_bits : int, optional\n",
      "    Shared weight width/precision used by weight. Mostly for fixed\n",
      "    point implementations. Unnecessary for floating point\n",
      "    implementations.\n",
      "    Default is for weights to use full 8 bit precision.\n",
      "\n",
      "sign_mode : SignMode, optional\n",
      "    Shared indicator whether synapse is of type SignMode.NULL,\n",
      "    SignMode.MIXED, SignMode.EXCITATORY, or SignMode.INHIBITORY. If\n",
      "    SignMode.MIXED, the sign of the weight is\n",
      "    included in the weight bits and the fixed point weight used for\n",
      "    inference is scaled by 2.\n",
      "    Unnecessary for floating point implementations.\n",
      "\n",
      "    In the fixed point implementation, weights are scaled according to\n",
      "    the following equations:\n",
      "    w_scale = 8 - num_weight_bits + weight_exp + isMixed()\n",
      "    weights = weights * (2 ** w_scale)\n",
      "\n",
      "num_message_bits : int, optional\n",
      "    Determines whether the Dense Process deals with the incoming\n",
      "    spikes as binary spikes (num_message_bits = 0) or as graded\n",
      "    spikes (num_message_bits > 0). Default is 0.\n",
      "    \n",
      "\u001b[0;31mInit docstring:\u001b[0m Initializes a new Process.\n",
      "\u001b[0;31mFile:\u001b[0m           /opt/conda/lib/python3.10/site-packages/lava/proc/dense/process.py\n",
      "\u001b[0;31mType:\u001b[0m           ProcessPostInitCaller\n",
      "\u001b[0;31mSubclasses:\u001b[0m     LearningDense, DelayDense"
     ]
    }
   ],
   "source": [
    "class LsnnNet(AbstractProcess):\n",
    "    def __init__(self, input_weights,\n",
    "                output_weights,\n",
    "                recurrent_weights,\n",
    "                **params) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.input = Dense(weights = input_weights)\n",
    "\n",
    "        self.forward_lif = LearningLIF(shape=input_weights.shape[0],\n",
    "                                       u = params.get(\"f_u\",1),\n",
    "                                       v = params.get(\"f_v\",0),\n",
    "                                       dv = params.get(\"f_dv\",0),\n",
    "                                       du = params.get(\"f_du\",1),\n",
    "                                       bias_exp = params.get(\"f_bias_exp\",1),\n",
    "                                       bias_mant = params.get(\"f_bias_mant\",1),\n",
    "                                       vth = params.get(\"f_vth\",1),\n",
    "                                       log_config = params.get(\"log_config\",1),\n",
    "                                       learning_rule = params.get(\"learning_rule\", None)\n",
    "                                       )\n",
    "        \n",
    "        self.output = Dense(weights = output_weights)\n",
    "\n",
    "        self.backword_alif = LearningLIF(shape = output_weights.shape[0],\n",
    "                                       u = params.get(\"r_u\",1),\n",
    "                                       v = 0,\n",
    "                                       du = params.get(\"r_du\",1),\n",
    "                                       dv = 1,\n",
    "                                       bias_exp = 0,\n",
    "                                       bias_mant = 1,\n",
    "                                       vth = 10000,\n",
    "                                       log_config = params.get(\"log_config\",1),\n",
    "                                       learning_rule = params.get(\"learning_rule\",None)\n",
    "                                       )\n",
    "          \n",
    "        self.recurrent = Dense(weights = recurrent_weights)\n",
    "\n",
    "        self.s_in  = InPort(shape = input_weights.shape[1], reduce_op = ReduceSum)\n",
    "        self.a_out  = OutPort(shape = output_weights.shape[0])\n",
    "        \n",
    "        self.s_in.connect(self.input.s_in)\n",
    "        self.output.a_out.connect(self.a_out)\n",
    "\n",
    "        self.forward_lif.a_in.connect_from([self.input.out_ports, self.recurrent])\n",
    "\n",
    "\n",
    "        self.forward_lif.s_out.connect(self.output.s_in)\n",
    "\n",
    "        self.output.a_out.connect(self.backword_alif.a_in)\n",
    "        self.backword_alif.s_out.connect(self.recurrent.s_in)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
